services:
  main_app:
    build:
      context: src
      dockerfile: dockerfiles/main_app.Dockerfile
      args: 
        BUILD_ENV: ${ENVIRONMENT}
        LOG_LEVEL: ${LOG_LEVEL}
    command:
      [
        "uvicorn",
        "--host=0.0.0.0",
        "--port=${BATCH_TS_APP_PORT}",
        "main_app.main:app",
      ]
    hostname: ${BATCH_TS_APP_HOSTNAME}
    profiles:
      - ""
      - all
      - batch_ts-dev
    ports:
      - "${BATCH_TS_APP_PORT}:${BATCH_TS_APP_PORT}"

  ollama:
    build:
      context: src
      dockerfile: dockerfiles/ollama_custom.Dockerfile
    ports:
      - 11433:11434
    volumes:
      #- ./ollama_entrypoint.sh:/entrypoint.sh # For Linux docker compose
      - ./ollama_entrypoint_windows.sh:/entrypoint.sh # For Windows docker compose
      - ./src/models:/root/.ollama/models # Local models directory mounted inside the container; Comment when loading for non-internet access
    container_name: rdai_ollama
    pull_policy: always
    tty: true
    restart: always
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]
    deploy:
      resources: 
        reservations: 
          devices: 
            - driver: nvidia
              count: 1 
              capabilities: [gpu]

  # nginx:
  #   image: nginx:1.15-alpine
  #   container_name: nginx
  #   volumes:
  #     - ./frontend/nginx.conf:/etc/nginx/nginx.conf
  #   ports:
  #     - 80:8888
  #   depends_on:
  #     - ui

  # ui:
  #   build:
  #     context: ./frontend/
  #     dockerfile: Dockerfile
  #   image: investigator-ui:1.1.0
  #   container_name: investigator-ui
  #   environment:
  #     - VITE_API_ENDPOINT=${VITE_API_ENDPOINT}
  #   ports:
  #     - 5173:5173
  #   command: npm run serve

